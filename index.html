<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Project Proposal: Application of Machine Learning in Heart Attack Risk Analysis</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
<style>
  :root{
    --brand:#1f4ed6;
    --ink:#111827;
    --muted:#6b7280;
    --paper:#ffffff;
    --bg:#f5f7fb;
    --card:#ffffff;
    --shadow:0 6px 18px rgba(17,24,39,.08);
  }
  html,body{height:100%;}
  body{
    margin:0; background:var(--bg); color:var(--ink);
    font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,"Helvetica Neue",Arial;
    line-height:1.6; font-size:16px;
  }
  .wrap{max-width:980px;margin:32px auto;padding:0 18px;}
  header{
    background:var(--paper); border-radius:14px; box-shadow:var(--shadow);
    padding:28px 28px 20px; margin-bottom:18px;
  }
  header h1{margin:0 0 6px; font-size:28px; font-weight:700; letter-spacing:.2px;}
  header .meta{color:var(--muted); font-size:14px;}
  .card{
    background:var(--card); border-radius:14px; box-shadow:var(--shadow);
    padding:26px 28px; margin:14px 0;
  }
  .content{counter-reset: section;}
  .section{counter-increment: section; counter-reset: subsection;}
  .section > h2::before{
    content: counter(section) ". ";
    color:var(--brand);
  }
  .section > h2{
    margin:0 0 14px; font-size:22px; letter-spacing:.2px;
    border-left:4px solid var(--brand); padding-left:10px;
  }
  .subsection{margin-top:14px; counter-increment: subsection;}
  .subsection > h3::before{
    content: counter(section) "." counter(subsection) " ";
    color:var(--brand);
  }
  .subsection > h3{
    margin:12px 0 6px; font-size:18px;
  }
  p{margin:8px 0;}
  a{color:var(--brand); text-decoration:none;}
  a:hover{text-decoration:underline;}
  ul{margin:6px 0 10px 18px;}
  .ref{color:var(--muted); font-size:14px;}
  .footer{color:var(--muted); text-align:center; font-size:13px; margin:22px 0 10px;}
  @media print{
    body{background:#fff;}
    header,.card{box-shadow:none;border:1px solid #e5e7eb;}
  }
  h4 {
  font-size: 18px;
  font-weight: 500;
  color: #333;
  margin-top: 1em;
  margin-bottom: 0.3em;
}
</style>
</head>
<body>
  <div class="wrap">
    <header>
      <h1>Project Midterm: Application of Machine Learning in Heart Attack Risk Analysis</h1>
      <div class="meta">Group 14 · November 2025</div>
    </header>

    <main class="content">
      <!-- 1. Introduction -->
      <section class="card section">
        <h2>Introduction</h2>

        <div class="subsection">
          <h3>Literature Review</h3>
          
          <p> Early research on cardiovascular risk prediction relied on population studies. The Framingham Risk Score showed that blood pressure and cholesterol categories could predict coronary heart disease. Later, the Reynolds Risk Score introduced new biomarkers. It included high-sensitivity C-reactive protein and family history. This addition improved risk classification for women.These scoring systems remain important in clinical practice. However, they rely on fixed categories and linear models. They cannot capture the complex and nonlinear interactions between many risk factors. Consequently, there is an urgent need for more accurate and interpretable prediction models.</p>

          <p><strong>Supervised machine learning.</strong> extends beyond traditional regression approaches. These methods learn directly from labeled data. Support Vector Machines (SVMs) create strong classification boundaries in high-dimensional spaces. Ensemble methods, such as Random Forests and Gradient Boosting models (XGBoost, LightGBM, CatBoost), often outperform single classifiers on heart-disease datasets. These models capture nonlinear patterns and interactions between features. Neural networks, especially Convolutional Neural Networks (CNNs), were first developed for image recognition. They are now adapted to analyze ECG signals and other physiological data for cardiac prediction. </p>

          <p><strong>Unsupervised methods.</strong> also assist researchers in identifying hidden patterns without using outcome labels. K-means clustering divides patient groups into subgroups that share similar characteristics. These clusters can reveal distinct cardiovascular phenotypes. Hierarchical clustering produces tree-like structures of subgroups. This approach is often highly interpretable in clinical practice. Dimensionality reduction methods, such as PCA and t-SNE, are also applied to cardiovascular data. They enable the visualization of risk profiles and uncover structure in high-dimensional features. These methods support exploratory risk stratification and provide insights that can guide feature engineering for supervised models.</p>
        </div>

        <div class="subsection">
          <h3>Dataset Description</h3>
          
          <p>The dataset is taken from Kaggle’s <a href="https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset" target="_blank">Heart Attack Prediction Dataset</a>. It includes patient demographics, medical history, lifestyle factors, and socioeconomic indicators. The target variable is Heart Attack Risk, labeled as high or low risk.</p>
        </div>
      </section>

      <!-- 2. Problem Definition -->
      <section class="card section">
      
        <h2>Problem Definition</h2>

        <div class="subsection">
          <h3>Problem</h3>
          <p>Heart attacks remain one of the leading causes of morbidity and mortality worldwide, accounting for millions of deaths each year. Despite advances in clinical diagnosis, many patients still lack timely risk assessment before symptoms appear, resulting in missed opportunities for early intervention. Traditional approaches, such as the Framingham Risk Score, rely heavily on limited clinical indicators (e.g., blood pressure, cholesterol) and physician expertise.  However, these factors are insufficient to capture the complex interactions among medical, lifestyle, and socioeconomic factors.</p>
        </div>

        <div class="subsection">
          <h3>Motivation</h3>
          <p>To effectively reduce the incidence and mortality of heart attacks, it is urgent to establish a data-driven prediction method that can integrate medical indicators, lifestyle factors, and socioeconomic environments to classify and predict patients’ risk of heart attacks. Machine learning provides an effective solution, as it can capture complex and nonlinear relationships among heterogeneous features. Such predictive models can support early diagnosis, enable personalized interventions, and ultimately assist healthcare providers in making informed clinical decisions.</p>
        </div>
      </section>


      <section class="card section">
        <h2>Methods</h2>
        <div class="subsection">
          <h3>Data Preprocessing</h3>
          <p>
          <strong>Data preprocessing</strong> was automated to ensure data consistency and improve model performance. The pipeline included <strong>blood pressure feature splitting</strong>, categorical <strong>encoding</strong>, and <strong>log transformations</strong> for skewed variables. A <strong>multi-scale feature scaling</strong> strategy was applied, comparing the performance of <em>StandardScaler</em>, <em>RobustScaler</em>, and <em>MinMaxScaler</em> to enhance model stability and convergence across different algorithms.
        </p>
        </div>
        <div class="subsection">
          <h3>Heart Attack Risk Triage</h3>
<p>
  <strong>Unsupervised learning methods</strong> were implemented to perform heart attack risk triage 
  and uncover hidden structure within the dataset. The main objective was to identify subgroups of patients 
  who share similar medical and lifestyle characteristics, providing a data-driven view of heart attack risk stratification.
</p>

<p>
  Three clustering algorithms were applied to achieve robust and diverse grouping outcomes: 
  <strong>K-Means</strong>, <strong>Gaussian Mixture Model (GMM)</strong>, and 
  <strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>. 
  These methods were selected to capture different types of data distributions — 
  K-Means for compact spherical clusters, GMM for probabilistic soft clustering, 
  and DBSCAN for discovering irregular shapes and outliers.
</p>
        </div>
        <div class="subsection">
          <h3>Heart Attack Risk Classification</h3>
          <p>
  <strong>Supervised learning methods</strong> were employed for heart attack risk classification. 
  Two nonlinear ensemble models, <strong>Random Forest</strong> and <strong>XGBoost</strong>, 
  were implemented to perform binary prediction of high- and low-risk patients.
</p>

<p>
  Both models were chosen for their ability to capture complex nonlinear interactions 
  between medical, demographic, and lifestyle features, which are often missed by traditional linear classifiers. 
  Extensive feature engineering and scaling were conducted prior to model training to ensure consistency across predictors.
</p>

<p>
  To address class imbalance in the dataset, the 
  <strong>SMOTE</strong> (Synthetic Minority Oversampling Technique) algorithm was applied, 
  generating synthetic samples for underrepresented classes and ensuring balanced representation 
  between minority and majority groups. 
  This approach enhanced model generalization and reduced bias toward the dominant class.
</p>

        </div>
      </section>
      <section class="card section">
      <h2>Results and Discussion</h2>
      <div class="subsection">
      <h3>Metrics</h3>
      <h4>Unsupervised Model Metrics</h4>
      <p>
  To identify the most stable cluster configuration, an 
  <strong>optimal cluster analysis</strong> was performed using automated 
  <em>k</em>-value optimization across multiple internal validation metrics. 
  The <strong>Elbow method</strong> and the <strong>Silhouette Score</strong> curve helped determine the appropriate 
  number of clusters for K-Means by balancing compactness and separation.
</p>

<figure style="text-align:center;">
  <img src="figures/optimal_k_analysis_KMeans.png" alt="Optimal K analysis" width="85%">
  <figcaption><strong>Figure 1.</strong> Determining the optimal number of clusters for K-Means using inertia, 
  Silhouette, Davies–Bouldin, and Calinski–Harabasz metrics.</figcaption>
</figure>

<p>
  After selecting the optimal <em>k</em>, a comprehensive comparison was made across 
  <strong>K-Means</strong>, <strong>GMM</strong>, and <strong>DBSCAN</strong> algorithms.  
  Evaluation metrics such as the <em>Silhouette Score</em>, <em>Davies–Bouldin Index</em>, and 
  <em>Calinski–Harabasz Score</em> were used to assess clustering quality from multiple perspectives.  
  This analysis demonstrated that K-Means achieved the best balance between cohesion and separation, 
  providing interpretable and consistent subgroup structures.
</p>

<figure style="text-align:center;">
  <img src="figures/clustering_evaluation_comparison.png" alt="Clustering algorithm comparison" width="85%">
  <figcaption><strong>Figure 2.</strong> Comparison of clustering performance across K-Means, GMM, and DBSCAN 
  using Silhouette, Davies–Bouldin, and Calinski–Harabasz metrics.</figcaption>
</figure>

<h4>Supervised Model Metrics</h4>
<p>
  To evaluate model robustness, hyperparameter tuning experiments were conducted for both Random Forest and XGBoost classifiers. 
  Validation accuracy was tracked across different parameter configurations to identify optimal depth and estimator settings.
</p>

<figure style="text-align:center;">
  <img src="figures/tuning_trends_RF.png" alt="Random Forest tuning trends" width="75%">
  <figcaption><strong>Figure 3.</strong> Validation accuracy of Random Forest across different numbers of estimators and maximum tree depths.</figcaption>
</figure>

<figure style="text-align:center;">
  <img src="figures/tuning_trends_XGB.png" alt="XGBoost parameter combinations" width="85%">
  <figcaption><strong>Figure 4.</strong> Top XGBoost parameter combinations ranked by accuracy and F1-score during hyperparameter optimization.</figcaption>
</figure>
      </div>
      <div class="subsection">
      <h3>Results</h3>
<h4>Unsupervised Model Results</h4>
<p>
  After identifying the optimal number of clusters, we analyzed the resulting subgroup structures using multiple visualization techniques. 
  These plots help interpret the underlying relationships between medical, lifestyle, and socioeconomic factors that contribute to different heart attack risk profiles.
</p>

<figure style="text-align:center;">
  <img src="figures/clustering_visualization.png" alt="PCA and t-SNE cluster visualization" width="90%">
  <figcaption><strong>Figure 5.</strong> PCA and t-SNE projections illustrating patient distributions across clusters for K-Means, GMM, and DBSCAN algorithms. 
  The visualization reveals that K-Means produces clearer separation boundaries compared to other methods.</figcaption>
</figure>

<p>
  To better understand cluster characteristics, feature-level visualizations were generated. 
  The <strong>feature heatmap</strong> below displays standardized feature means across each K-Means cluster, highlighting differences in lifestyle and clinical indicators such as 
  <em>smoking habits</em>, <em>exercise hours</em>, and <em>blood pressure</em>.
</p>

<figure style="text-align:center;">
  <img src="figures/cluster_features_heatmap_KMeans.png" alt="Cluster feature heatmap" width="90%">
  <figcaption><strong>Figure 6.</strong> Standardized feature averages across K-Means clusters, showing distinct profiles in behavior and medical conditions.</figcaption>
</figure>

<p>
  We also examined the overall <strong>risk distribution</strong> and <strong>sample composition</strong> within each cluster. 
  The following plots illustrate that each cluster maintains a balanced number of samples, while the proportion of high-risk individuals varies slightly across groups.
</p>

<figure style="text-align:center;">
  <img src="figures/cluster_characteristics_KMeans.png" alt="Cluster risk and sample size" width="90%">
  <figcaption><strong>Figure 7.</strong> Distribution of heart attack risk and sample sizes across clusters for K-Means.</figcaption>
</figure>

<p>
  Finally, a <strong>radar chart</strong> comparison was conducted to visualize multidimensional feature differences among clusters. 
  The results show that certain clusters are characterized by higher stress levels, lower exercise frequency, and increased alcohol consumption, 
  indicating different behavioral risk patterns among patients.
</p>

<figure style="text-align:center;">
  <img src="figures/cluster_radar_chart_KMeans.png" alt="Radar chart cluster comparison" width="85%">
  <figcaption><strong>Figure 8.</strong> Radar chart comparison showing average feature profiles for each K-Means cluster.</figcaption>
</figure>
<h4>Supervised Model Results</h4>
<p>
  The final evaluation was performed on the test set using the optimized models. 
  The confusion matrices below summarize the classification outcomes for high- and low-risk predictions. 
  Both Random Forest and XGBoost achieved moderate overall accuracy, with XGBoost showing slightly better class balance.
</p>

<figure style="text-align:center;">
  <img src="figures/confusion_matrix_RF.png" alt="Random Forest confusion matrix" width="45%">
  <img src="figures/confusion_matrix_XGB.png" alt="XGBoost confusion matrix" width="45%">
  <figcaption><strong>Figure 9.</strong> Confusion matrices for Random Forest (left) and XGBoost (right) models on the test set.</figcaption>
</figure>
      </div>
      </section>
  <section class="card section">
<h3>References</h3>

<ol>
  <li>World Health Organization. <em>Cardiovascular diseases (CVDs)</em>, 2021. 
    <a href="https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)" target="_blank">[Online]</a>
  </li>

  <li>Wilson, P. W., D’Agostino, R. B., Levy, D., Belanger, A. M., Silbershatz, H., & Kannel, W. B. 
    <em>Prediction of coronary heart disease using risk factor categories.</em> 
    <strong>Circulation</strong>, 97(18), 1837–1847, 1998. 
    <a href="https://doi.org/10.1161/01.cir.97.18.1837" target="_blank">https://doi.org/10.1161/01.cir.97.18.1837</a>
  </li>

  <li>Ridker, P. M., Buring, J. E., Rifai, N., & Cook, N. R. 
    <em>Development and validation of improved algorithms for the assessment of global cardiovascular risk in women: the Reynolds Risk Score.</em> 
    <strong>JAMA</strong>, 297(6), 611–619, 2007. 
    <a href="https://doi.org/10.1001/jama.297.6.611" target="_blank">https://doi.org/10.1001/jama.297.6.611</a>
  </li>

  <li>D’Agostino, R. B., Vasan, R. S., Pencina, M. J., Wolf, P. A., Cobain, M., Massaro, J. M., & Kannel, W. B. 
    <em>General cardiovascular risk profile for use in primary care: The Framingham Heart Study.</em> 
    <strong>Circulation</strong>, 117(6), 743–753, 2008.
  </li>

  <li>Hearst, M. A., Dumais, S. T., Osuna, E., Platt, J., & Schölkopf, B. 
    <em>Support vector machines.</em> 
    <strong>IEEE Intelligent Systems and Their Applications</strong>, 13(4), 18–28, 1998. 
    <a href="https://doi.org/10.1109/5254.708428" target="_blank">https://doi.org/10.1109/5254.708428</a>
  </li>

  <li>O’Shea, K., & Nash, R. 
    <em>An Introduction to Convolutional Neural Networks.</em> 
    <strong>arXiv</strong>, 1511.08458, 2015. 
    <a href="https://arxiv.org/abs/1511.08458" target="_blank">arXiv:1511.08458</a>
  </li>

  <li>Dorogush, A. V., Gulin, A., Gusev, G., Kazeev, N., Prokhorenkova, L. O., & Vorobev, A. 
    <em>Fighting biases with dynamic boosting.</em> 
    <strong>arXiv</strong>, 1706.09516, 2017. 
    <a href="https://arxiv.org/abs/1706.09516" target="_blank">arXiv:1706.09516</a>
  </li>

  <li>MacQueen, J. 
    <em>Multivariate observations.</em> 
    In <strong>Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability</strong>, Vol. 1, pp. 281–297, 1967.
  </li>

  <li>e1010101. 
    <em>Heart Attack Risk Analysis.</em> 
    Kaggle, 2023. 
    <a href="https://www.kaggle.com/competitions/heart-attack-risk-analysis" target="_blank">[Dataset Link]</a>
  </li>
     
</section>
        <section class="card section">
       <h3>5. Contribution Table</h3>
        <table style="width: 100%; border-collapse: collapse; margin-top: 20px; background-color: white; border: 1px solid #ddd;">
            <thead>
                <tr style="background-color: #34495e; color: white;">
                    <th style="padding: 12px; text-align: left; border: 1px solid #ddd; width: 25%;">Name</th>
                    <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Midterm Contributions</th>
                </tr>
            </thead>
            <tbody>
                <tr style="background-color: #f8f9fa;">
                    <td style="padding: 12px; border: 1px solid #ddd; font-weight: 500;">Jiahui Zhang</td>
                    <td style="padding: 12px; border: 1px solid #ddd;"> Revised the final report and organized project documentation. Responsible for creating the contribution table, GitHub Page, and project Gantt chart.</td>
                </tr>
                <tr>
                    <td style="padding: 12px; border: 1px solid #ddd; font-weight: 500;">Qinxuan Li</td>
                    <td style="padding: 12px; border: 1px solid #ddd;"> Developed the supervised learning models and contributed to model visualization and performance metrics.</td>
                </tr>
                <tr style="background-color: #f8f9fa;">
                    <td style="padding: 12px; border: 1px solid #ddd; font-weight: 500;">Jiannan Huang</td>
                    <td style="padding: 12px; border: 1px solid #ddd;">Conducted model evaluation and performance analysis, summarizing insights and interpreting experimental results.</td>
                </tr>
                <tr>
                    <td style="padding: 12px; border: 1px solid #ddd; font-weight: 500;">Jialuo Li</td>
                    <td style="padding: 12px; border: 1px solid #ddd;">Designed and implemented the overall visualization framework, ensuring clarity and consistency across plots and figures.</td>
                </tr>
                <tr style="background-color: #f8f9fa;">
                    <td style="padding: 12px; border: 1px solid #ddd; font-weight: 500;">Binfei Ji</td>
                    <td style="padding: 12px; border: 1px solid #ddd;">Implemented the unsupervised learning models and contributed to cluster visualization and evaluation metrics.</td>
                </tr>
            </tbody>
          </section>
        <div class="gantt-section">
            <h2>Project Timeline (Gantt Chart)</h2>
            <p style="color: #7f8c8d; font-size: 0.9em; margin-bottom: 20px;">Timeline loaded from GanttChart.xlsx</p>

            <div id="gantt-container" style="overflow-x: auto;">
                <p style="padding: 20px; color: #7f8c8d;">Loading Gantt chart...</p>
            </div>
        </div>
    </div>

    <!-- SheetJS library for reading Excel files -->
    <script src="https://cdn.sheetjs.com/xlsx-0.20.1/package/dist/xlsx.full.min.js"></script>

    <script>
        // Convert Excel date number to readable date string
        function excelDateToString(excelDate) {
            if (typeof excelDate === 'string') return excelDate;
            if (!excelDate || excelDate === '') return '';

            let date;
            if (excelDate instanceof Date) {
                // Already a Date object
                date = excelDate;
            } else if (typeof excelDate === 'number') {
                // Excel dates are days since 1900-01-01 (with a bug for Feb 29, 1900)
                date = new Date((excelDate - 25569) * 86400 * 1000);
            } else {
                return String(excelDate);
            }

            const month = String(date.getMonth() + 1).padStart(2, '0');
            const day = String(date.getDate()).padStart(2, '0');
            return `${month}/${day}`;
        }

        function isRowEmpty(row) {
            return row.every(cell => !cell || cell === '');
        }

        async function loadGanttChart() {
            try {
                const response = await fetch('GanttChart.xlsx');
                const arrayBuffer = await response.arrayBuffer();
                const workbook = XLSX.read(arrayBuffer, { type: 'array', cellStyles: true });

                const sheetName = 'Fall';
                const worksheet = workbook.Sheets[sheetName];
                const data = XLSX.utils.sheet_to_json(worksheet, { header: 1, defval: '' });
                const merges = worksheet['!merges'] || [];

                let html = '<table style="width: max-content; min-width: 100%; border-collapse: collapse; background-color: white; border: 1px solid #ddd;">';

                data.forEach((row, rowIndex) => {
                    if (rowIndex === 0 || isRowEmpty(row)) return;

                    html += '<tr>';
                    row.forEach((cell, cellIndex) => {
                        const isHeader = rowIndex <= 6;
                        const cellRef = XLSX.utils.encode_cell({ r: rowIndex, c: cellIndex });
                        const cellObj = worksheet[cellRef];

                        let shouldSkip = false;
                        let colspan = 1;
                        let rowspan = 1;

                        for (const merge of merges) {
                            if (rowIndex >= merge.s.r && rowIndex <= merge.e.r &&
                                cellIndex >= merge.s.c && cellIndex <= merge.e.c) {
                                if (rowIndex === merge.s.r && cellIndex === merge.s.c) {
                                    colspan = merge.e.c - merge.s.c + 1;
                                    rowspan = merge.e.r - merge.s.r + 1;
                                } else {
                                    shouldSkip = true;
                                }
                                break;
                            }
                        }

                        if (shouldSkip) return;

                        let displayValue = cell || '';
                        if (cell instanceof Date) {
                            displayValue = excelDateToString(cell);
                        } else if (typeof cell === 'number' && (cellIndex === 3 || cellIndex === 4 || cellIndex === 5)) {
                            if (cell > 40000) {
                                displayValue = excelDateToString(cell);
                            } else {
                                displayValue = cell;
                            }
                        }

                        let bgColor = '';
                        if (cellObj && cellObj.s && cellObj.s.fgColor) {
                            const rgb = cellObj.s.fgColor.rgb;
                            if (rgb) {
                                bgColor = `#${rgb}`;
                            }
                        }

                        let style = '';
                        if (cellIndex === 1) {
                            style = isHeader
                                ? 'padding: 8px; border: 1px solid #ddd; background-color: #34495e; color: white; font-weight: bold; text-align: center; min-width: 200px; max-width: 300px; word-wrap: break-word;'
                                : `padding: 8px; border: 1px solid #ddd; min-width: 200px; max-width: 300px; word-wrap: break-word; ${bgColor ? `background-color: ${bgColor};` : ''}`;
                        } else if (cellIndex === 2) {
                            style = isHeader
                                ? 'padding: 8px; border: 1px solid #ddd; background-color: #34495e; color: white; font-weight: bold; text-align: center; min-width: 80px; max-width: 120px;'
                                : `padding: 8px; border: 1px solid #ddd; min-width: 80px; max-width: 120px; ${bgColor ? `background-color: ${bgColor};` : ''}`;
                        } else {
                            style = isHeader
                                ? 'padding: 8px; border: 1px solid #ddd; background-color: #34495e; color: white; font-weight: bold; text-align: center; min-width: 50px;'
                                : `padding: 8px; border: 1px solid #ddd; min-width: 50px; ${bgColor ? `background-color: ${bgColor};` : ''}`;
                        }

                        const tag = isHeader ? 'th' : 'td';
                        const colspanAttr = colspan > 1 ? ` colspan="${colspan}"` : '';
                        const rowspanAttr = rowspan > 1 ? ` rowspan="${rowspan}"` : '';
                        html += `<${tag} style="${style}"${colspanAttr}${rowspanAttr}>${displayValue}</${tag}>`;
                    });
                    html += '</tr>';
                });

                html += '</table>';
                document.getElementById('gantt-container').innerHTML = html;

            } catch (error) {
                console.error('Error loading Gantt chart:', error);
                document.getElementById('gantt-container').innerHTML =
                    '<p style="color: red; padding: 20px;">Error loading chart. Make sure GanttChart.xlsx is available.</p>';
            }
        }

        document.addEventListener('DOMContentLoaded', loadGanttChart);
    </script>
       <p class="footer">© 2025 Group 14 · This page is a static site built for course project midterm.</p>
    </main>
  </div>
</body>
</html>
